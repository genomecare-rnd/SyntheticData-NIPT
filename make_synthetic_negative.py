import os
import subprocess
import random
import numpy as np
import pandas as pd
from tqdm import tqdm
import multiprocessing
from itertools import combinations
from utils.preprocess_fastq import extract_unique_reads

def get_filename_from_path(file_path):
    """
    Extract filename without extension from full file path
    
    Parameters
    ----------
    file_path : str
        Full file path (e.g., /SyntheticDataGeneration/Demo_samples/Sample_male_1.Fastq)
        
    Returns
    -------
    str
        Filename without extension (e.g., Sample_male_1)
    """
    return os.path.splitext(os.path.basename(file_path))[0]

def make_negative(fastq_dfs, filenames):
    """
    Description
    -------
    Create one synthetic negative sample by mixing multiple real FASTQ-derived
    unique-read tables, then compute the fetal fraction (FF) of the mixture
    as a read-count–weighted average.
    
    - Synthetic negatives are generated by randomly merging ≥2 real FASTQ files.
    - The fetal fraction (FF) of the merged sample is recalculated by weighting
      each source FF according to its number of unique reads (UR).
    - After merging, reads are randomly shuffled, and a subset is sampled to
      approximate the desired number of total reads.
    - These synthetic negatives are then used as input for downstream model
      training and evaluation.

    Inputs
    ------
    fastq_dfs : dict
        { filename: {"DATA": DataFrame, "FF": float} }
        - "DATA": table of unique mapped reads from that FASTQ
        - "FF": fetal fraction estimated for that sample
    filenames : list[str]
        The set of sample IDs to combine for one synthetic negative.

    Returns
    -------
    df_new : pd.DataFrame
        The merged-and-shuffled unique-read table, downsampled to a fixed ratio.
    FF_new : float
        Weighted-average FF for the merged sample.
    filename_new : str
        Output filename embedding the components and FF value.
    """
    dfs = []
    comb_UR = 0
    weighted_FF = 0
    
    # Collect each unique-read table and its FF; accumulate weighted terms.
    # UR (number of unique reads) serves as the weight for FF averaging.
    for filename in filenames:
        df = fastq_dfs[filename]['DATA']
        FF = fastq_dfs[filename]['FF']
        UR = df.shape[0]  # number of unique reads
        
        dfs.append(df)
        comb_UR += UR
        weighted_FF += (UR * FF)
    
    # Weighted average fetal fraction of the mixture
    FF_new = weighted_FF / comb_UR
    
    # Merge all unique-read tables and randomly shuffle
    merged_df = pd.concat(dfs, ignore_index=True)
    merged_df = merged_df.sample(frac=1).reset_index(drop=True)

    # Downsample to a fixed ratio so that synthetic sample size remains controlled
    sampling_ratio = 1 / len(filenames)
    df_new = merged_df[:int(len(merged_df) * sampling_ratio)]
    
    # Descriptive filename for traceability
    combined_names = '_'.join(filenames)
    filename_new = f'SynNeg_{combined_names}_{FF_new:.2f}.csv'
    
    return df_new, FF_new, filename_new

if __name__ == "__main__":
    # The manuscript generated negatives within male/female groups.
    gender = 'male'
    
    # Set output directory (can be changed as needed)
    output_dir = './synthetic_negatives'
    
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    print(f"Output directory: {os.path.abspath(output_dir)}")

    # Load per-sample FF values
    sample_data = pd.read_csv('Sample_FF_info.csv')
    file_paths = sample_data['sample_id'].tolist()  # All file paths
    filenames = [get_filename_from_path(path) for path in file_paths]  # Extract filenames only
    
    all_combinations = []
    
    # Generate all combinations of 2..N files, as required by the algorithm
    for comb_number in range(2,len(filenames)+1):
        all_combinations += list(combinations(filenames, comb_number))

    # Create dictionary mapping filenames to paths
    filename_to_path = {get_filename_from_path(path): path for path in file_paths}
    
    # Process each combination to produce one synthetic negative sample
    combination_desc = f"Processing combinations"
    for fastq_combination in tqdm(all_combinations, desc=combination_desc):
        fastq_dfs = {}
        
        for filename in fastq_combination:
            # Get original file path
            original_file_path = filename_to_path[filename]
            
            # Create unique FASTQ file path in the same directory as original file
            original_dir = os.path.dirname(original_file_path)
            unique_fastq_path = os.path.join(original_dir, f"{filename}.unique.Fastq")
            
            # Ensure unique-read FASTQ exists; if not, extract unique reads to preprocess_fastq.py
            if not os.path.exists(unique_fastq_path):
                # Extract unique reads using original file path
                unique_file = extract_unique_reads(original_file_path, './UCSC_hg19/index', threads=12, keep_temp=False)

            # Load essential fields: chromosome, position, sequence from unique-read FASTQ
            unique_df = pd.read_table(
                unique_fastq_path,
                header=None, usecols=[2, 3, 9],
                names=['Chr', 'Pos', 'Seq'],
                dtype={'Chr': 'str', 'Pos': 'int32', 'Seq': 'str'}
            )

            # Normalize X and Y labels to numeric (23, 24)
            unique_df['Chr'] = unique_df['Chr'].replace(['chrX', 'chrY'], ['chr23', 'chr24'])

            # Retrieve fetal fraction for this sample (search by original path)
            FF = sample_data[sample_data['sample_id'] == original_file_path]['FF'].iloc[0]
            
            fastq_dfs[filename] = {'DATA': unique_df, 'FF': FF}

        # Generate synthetic negative for this combination
        df_new, FF_new, filename_new = make_negative(fastq_dfs, list(fastq_combination))
        
        # Compute global features
        UR_new = df_new.shape[0]  # total unique reads
        ATRC_new = df_new[(df_new['Chr'] != 'chr23') & (df_new['Chr'] != 'chr24')].shape[0]  # autosomal reads only
        
        # GC ratio across all reads
        GC_count_new = df_new["Seq"].apply(lambda x: x.count("G") + x.count("C")).sum()
        total_seq_new = df_new["Seq"].apply(lambda x: len(x)).sum()
        GC_ratio_new = GC_count_new / total_seq_new
            
        # Reads from sex chromosomes
        chr23_count_new = df_new[df_new['Chr'] == 'chr23'].shape[0]
        chr24_count_new = df_new[df_new['Chr'] == 'chr24'].shape[0]
        
        # Per-chromosome read counts (1..24)
        chr_read_counts = []
        for i in range(1, 25):
            chr_count = len(df_new[df_new['Chr'] == f'chr{i}'])
            chr_read_counts.append(chr_count)

        # Construct final feature row (sample ID, metadata, global features, per-chromosome counts)
        chr_col_names = [f'chr{i}' for i in range(1, 25)]

        final_col_names = ['sample_id', 'gender', 'result', 'GC', 'UR', 'ATRC', 'FF',] + chr_col_names
        final_df = pd.DataFrame(np.array([[filename_new, gender, 'negative', GC_ratio_new, UR_new, ATRC_new, FF_new,] + chr_read_counts]))
        final_df.columns = final_col_names
        
        # Logging for progress monitoring
        combination_str = ' + '.join(fastq_combination)
        print(f"Processing combination: {combination_str}")
        print(f"FF: {FF_new:.4f}")
        print(f"Shape: {df_new.shape}")
        
        # Save synthetic negative as CSV in the specified output directory
        output_path = os.path.join(output_dir, filename_new)
        final_df.to_csv(output_path, index=False)
        print(f"Saved file: {output_path}")
